#!/usr/bin/env python3
"""
QwenåµŒå…¥æ¨¡å‹æµ‹è¯•è„šæœ¬
==================

ä¸“é—¨ç”¨äºæµ‹è¯•Qwen/Qwen3-Embedding-8BåµŒå…¥æ¨¡å‹çš„åŠŸèƒ½ã€‚

ä½œè€…ï¼šxrzlizheng
ç‰ˆæœ¬ï¼š1.0
"""

import os
import time
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import numpy as np

def test_qwen_embedding():
    """æµ‹è¯•QwenåµŒå…¥æ¨¡å‹"""
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    embedding_model = os.getenv("EMBEDDING_MODEL", "Qwen/Qwen3-Embedding-8B")
    
    print("ğŸ”§ QwenåµŒå…¥æ¨¡å‹æµ‹è¯•")
    print("=" * 50)
    print(f"ğŸ“‹ æ¨¡å‹åç§°: {embedding_model}")
    
    try:
        # åŠ è½½æ¨¡å‹
        print("\nğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        start_time = time.time()
        model = SentenceTransformer(embedding_model)
        load_time = time.time() - start_time
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼è€—æ—¶: {load_time:.2f}ç§’")
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
            "ç”¨æˆ·åå¥½æŸ¥çœ‹åˆ©ç‡ä½äº5%çš„è´·æ¬¾",
            "é«˜é£é™©è´·æ¬¾å®šä¹‰ä¸ºä¿¡ç”¨è¯„åˆ†ä½äº650ä¸”è´·æ¬¾é‡‘é¢è¶…è¿‡$200,000",
            "WCè¡¨ç¤ºè¥¿æµ·å²¸å·ï¼šCAã€ORå’ŒWA",
            "æ˜¾ç¤ºæ‰€æœ‰VIPå®¢æˆ·çš„ä¿¡æ¯"
        ]
        
        print(f"\nğŸ” æµ‹è¯•{len(test_texts)}ä¸ªæ–‡æœ¬çš„åµŒå…¥...")
        
        # ç”ŸæˆåµŒå…¥
        start_time = time.time()
        embeddings = model.encode(test_texts)
        encode_time = time.time() - start_time
        
        print(f"âœ… åµŒå…¥ç”ŸæˆæˆåŠŸï¼è€—æ—¶: {encode_time:.2f}ç§’")
        print(f"ğŸ“Š åµŒå…¥å‘é‡ç»´åº¦: {embeddings.shape[1]}")
        print(f"ğŸ“Š åµŒå…¥å‘é‡æ•°é‡: {embeddings.shape[0]}")
        
        # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
        print("\nğŸ” æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—...")
        
        # è®¡ç®—ç¬¬ä¸€ä¸ªæ–‡æœ¬ä¸å…¶ä»–æ–‡æœ¬çš„ç›¸ä¼¼åº¦
        query_embedding = embeddings[0]
        similarities = []
        
        for i, text in enumerate(test_texts[1:], 1):
            similarity = np.dot(query_embedding, embeddings[i]) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(embeddings[i])
            )
            similarities.append((i, text, similarity))
        
        print("ğŸ“Š ç›¸ä¼¼åº¦ç»“æœï¼ˆç›¸å¯¹äºç¬¬ä¸€ä¸ªæ–‡æœ¬ï¼‰:")
        for i, text, sim in similarities:
            print(f"  æ–‡æœ¬{i}: {text[:30]}... - ç›¸ä¼¼åº¦: {sim:.4f}")
        
        # æµ‹è¯•å•æ–‡æœ¬åµŒå…¥
        print("\nğŸ” æµ‹è¯•å•æ–‡æœ¬åµŒå…¥...")
        single_text = "è¿™æ˜¯ä¸€ä¸ªå•ç‹¬çš„æµ‹è¯•æ–‡æœ¬"
        single_embedding = model.encode(single_text)
        print(f"âœ… å•æ–‡æœ¬åµŒå…¥æˆåŠŸï¼ç»´åº¦: {len(single_embedding)}")
        
        # æµ‹è¯•æ‰¹å¤„ç†
        print("\nğŸ” æµ‹è¯•æ‰¹å¤„ç†åµŒå…¥...")
        batch_texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3", "æ–‡æœ¬4", "æ–‡æœ¬5"]
        batch_embeddings = model.encode(batch_texts)
        print(f"âœ… æ‰¹å¤„ç†åµŒå…¥æˆåŠŸï¼å½¢çŠ¶: {batch_embeddings.shape}")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼QwenåµŒå…¥æ¨¡å‹å·¥ä½œæ­£å¸¸ã€‚")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        print("\nğŸ“‹ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š")
        print("1. ç¡®ä¿å·²å®‰è£…sentence-transformers: pip install sentence-transformers")
        print("2. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œå¯ä»¥ä¸‹è½½æ¨¡å‹")
        print("3. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
        print("4. ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜å’Œç£ç›˜ç©ºé—´")
        return False

def test_embedding_performance():
    """æµ‹è¯•åµŒå…¥æ€§èƒ½"""
    
    print("\nğŸš€ æ€§èƒ½æµ‹è¯•")
    print("=" * 30)
    
    try:
        embedding_model = os.getenv("EMBEDDING_MODEL", "Qwen/Qwen3-Embedding-8B")
        model = SentenceTransformer(embedding_model)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_texts = [f"è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºæ€§èƒ½æµ‹è¯•ã€‚" for i in range(100)]
        
        print(f"ğŸ“Š æµ‹è¯•{len(test_texts)}ä¸ªæ–‡æœ¬çš„åµŒå…¥æ€§èƒ½...")
        
        # æµ‹è¯•å•æ–‡æœ¬æ€§èƒ½
        start_time = time.time()
        for text in test_texts[:10]:
            model.encode(text)
        single_time = time.time() - start_time
        print(f"â±ï¸  å•æ–‡æœ¬å¤„ç†ï¼ˆ10ä¸ªï¼‰: {single_time:.2f}ç§’")
        
        # æµ‹è¯•æ‰¹å¤„ç†æ€§èƒ½
        start_time = time.time()
        model.encode(test_texts)
        batch_time = time.time() - start_time
        print(f"â±ï¸  æ‰¹å¤„ç†ï¼ˆ{len(test_texts)}ä¸ªï¼‰: {batch_time:.2f}ç§’")
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        single_avg = single_time / 10
        batch_avg = batch_time / len(test_texts)
        speedup = single_avg / batch_avg if batch_avg > 0 else 0
        
        print(f"ğŸ“ˆ æ‰¹å¤„ç†åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"ğŸ“ˆ å¹³å‡å¤„ç†é€Ÿåº¦: {len(test_texts)/batch_time:.1f} æ–‡æœ¬/ç§’")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹QwenåµŒå…¥æ¨¡å‹æµ‹è¯•\n")
    
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    basic_ok = test_qwen_embedding()
    
    if basic_ok:
        # æ€§èƒ½æµ‹è¯•
        performance_ok = test_embedding_performance()
        
        if performance_ok:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼QwenåµŒå…¥æ¨¡å‹é…ç½®æ­£ç¡®ä¸”æ€§èƒ½è‰¯å¥½ã€‚")
            print("æ‚¨ç°åœ¨å¯ä»¥åœ¨æ™ºèƒ½æŸ¥è¯¢åŠ©æ‰‹ä¸­ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹äº†ã€‚")
        else:
            print("\nâš ï¸  åŸºæœ¬åŠŸèƒ½æ­£å¸¸ï¼Œä½†æ€§èƒ½æµ‹è¯•å¤±è´¥ã€‚")
    else:
        print("\nâŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
        print("\nğŸ“‹ é…ç½®æ£€æŸ¥æ¸…å•ï¼š")
        print("1. ç¡®ä¿å·²å®‰è£…sentence-transformers")
        print("2. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
        print("3. ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜å’Œç£ç›˜ç©ºé—´")
        print("4. æ£€æŸ¥EMBEDDING_MODELç¯å¢ƒå˜é‡è®¾ç½®")

if __name__ == "__main__":
    main() 